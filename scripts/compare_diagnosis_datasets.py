"""
Dataset Comparison for Differential Diagnosis Generation.
Tests emrqa-msquad vs epfl-llm/guidelines for clinical note ‚Üí diagnosis pipeline.

Goal: Determine which dataset best supports:
1. Clinical note summarization
2. Differential diagnosis generation
3. Evidence traceability
4. RAG integration
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from datasets import load_dataset
import json
from typing import List, Dict

def load_and_analyze_emrqa():
    """Load and analyze EMR-QA dataset for differential diagnosis."""
    print("="*100)
    print("DATASET 1: EMR-QA (Eladio/emrqa-msquad)")
    print("="*100)
    print("\nüìä Purpose: EMR Question-Answering for clinical reasoning\n")
    
    try:
        print("Loading emrqa-msquad dataset...")
        ds = load_dataset("Eladio/emrqa-msquad")
        
        print(f"\n‚úÖ Loaded successfully!")
        print(f"Structure: {ds}")
        
        split = list(ds.keys())[0]
        data = ds[split]
        
        print(f"\nSplit: {split}")
        print(f"Examples: {len(data)}")
        print(f"Features: {data.features}")
        
        # Show first example
        print(f"\n{'='*100}")
        print("SAMPLE EXAMPLE")
        print(f"{'='*100}")
        
        example = data[0]
        for key, value in example.items():
            print(f"\n{key.upper()}:")
            if isinstance(value, str) and len(value) > 300:
                print(f"{value[:300]}...")
            else:
                print(value)
        
        # Analyze for differential diagnosis use
        print(f"\n{'='*100}")
        print("SUITABILITY FOR DIFFERENTIAL DIAGNOSIS")
        print(f"{'='*100}")
        
        scores = {
            'has_clinical_context': False,
            'has_diagnosis_questions': False,
            'has_multiple_diagnoses': False,
            'has_evidence_links': False,
            'supports_summarization': False
        }
        
        # Check fields
        example_keys = list(example.keys())
        
        if any(k in example_keys for k in ['context', 'clinical_note', 'text', 'passage']):
            scores['has_clinical_context'] = True
            print("‚úÖ Contains clinical context/notes")
        
        if any(k in example_keys for k in ['question', 'query']):
            scores['has_diagnosis_questions'] = True
            print("‚úÖ Contains questions (good for diagnostic reasoning)")
        
        if any(k in example_keys for k in ['answer', 'answers']):
            scores['has_evidence_links'] = True
            print("‚úÖ Has answers (can link evidence to diagnosis)")
        
        # Check content
        for key, value in example.items():
            if isinstance(value, str):
                if 'diagnos' in value.lower():
                    scores['has_multiple_diagnoses'] = True
                if 'summary' in value.lower() or 'history' in value.lower():
                    scores['supports_summarization'] = True
        
        if scores['has_multiple_diagnoses']:
            print("‚úÖ Contains diagnostic content")
        if scores['supports_summarization']:
            print("‚úÖ Supports clinical summarization")
        
        suitability_score = sum(scores.values()) / len(scores) * 100
        print(f"\nüìä Suitability Score: {suitability_score:.0f}%")
        
        return ds, split, scores
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return None, None, {}


def load_and_analyze_guidelines():
    """Load and analyze clinical guidelines dataset."""
    print(f"\n\n{'='*100}")
    print("DATASET 2: Clinical Guidelines (epfl-llm/guidelines)")
    print("="*100)
    print("\nüìä Purpose: Evidence-based clinical practice guidelines\n")
    
    try:
        print("Loading guidelines dataset...")
        ds = load_dataset("epfl-llm/guidelines")
        
        print(f"\n‚úÖ Loaded successfully!")
        print(f"Structure: {ds}")
        
        split = list(ds.keys())[0]
        data = ds[split]
        
        print(f"\nSplit: {split}")
        print(f"Guidelines: {len(data)}")
        print(f"Features: {data.features}")
        
        # Show first example
        print(f"\n{'='*100}")
        print("SAMPLE GUIDELINE")
        print(f"{'='*100}")
        
        example = data[0]
        for key, value in example.items():
            print(f"\n{key.upper()}:")
            if isinstance(value, str) and len(value) > 300:
                print(f"{value[:300]}...")
            elif isinstance(value, list):
                print(f"[List with {len(value)} items]")
            else:
                print(value)
        
        # Analyze for differential diagnosis use
        print(f"\n{'='*100}")
        print("SUITABILITY FOR DIFFERENTIAL DIAGNOSIS")
        print(f"{'='*100}")
        
        scores = {
            'has_diagnostic_criteria': False,
            'has_conditions': False,
            'has_evidence_levels': False,
            'has_differential_info': False,
            'supports_rag': False
        }
        
        example_keys = list(example.keys())
        
        if any(k in example_keys for k in ['condition', 'disease', 'diagnosis', 'topic']):
            scores['has_conditions'] = True
            print("‚úÖ Contains condition/disease information")
        
        if any(k in example_keys for k in ['evidence_level', 'grade', 'strength', 'quality']):
            scores['has_evidence_levels'] = True
            print("‚úÖ Has evidence strength levels")
        
        if any(k in example_keys for k in ['content', 'text', 'guideline', 'recommendation']):
            scores['supports_rag'] = True
            print("‚úÖ Rich text content (good for RAG)")
        
        # Check content
        for key, value in example.items():
            if isinstance(value, str):
                if 'criteria' in value.lower() or 'diagnostic' in value.lower():
                    scores['has_diagnostic_criteria'] = True
                if 'differential' in value.lower():
                    scores['has_differential_info'] = True
        
        if scores['has_diagnostic_criteria']:
            print("‚úÖ Contains diagnostic criteria")
        if scores['has_differential_info']:
            print("‚úÖ Has differential diagnosis information")
        
        suitability_score = sum(scores.values()) / len(scores) * 100
        print(f"\nüìä Suitability Score: {suitability_score:.0f}%")
        
        return ds, split, scores
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return None, None, {}


def compare_datasets(emrqa_scores, guidelines_scores):
    """Direct comparison for differential diagnosis use case."""
    print(f"\n\n{'='*100}")
    print("COMPREHENSIVE COMPARISON")
    print(f"{'='*100}")
    
    print(f"\n{'Criteria':<40} {'EMR-QA':<15} {'Guidelines':<15} {'Winner'}")
    print("-"*100)
    
    comparisons = [
        ("Clinical Context/Notes", 
         "‚úÖ Direct" if emrqa_scores.get('has_clinical_context') else "‚ùå None",
         "‚ö†Ô∏è Indirect" if guidelines_scores.get('has_conditions') else "‚ùå None",
         "EMR-QA"),
        
        ("Diagnostic Questions",
         "‚úÖ Yes" if emrqa_scores.get('has_diagnosis_questions') else "‚ùå No",
         "‚ùå No",
         "EMR-QA"),
        
        ("Evidence Traceability",
         "‚úÖ Answer links" if emrqa_scores.get('has_evidence_links') else "‚ùå No",
         "‚úÖ Citations" if guidelines_scores.get('has_evidence_levels') else "‚ùå No",
         "Tie"),
        
        ("Diagnostic Criteria",
         "‚ö†Ô∏è Implicit",
         "‚úÖ Explicit" if guidelines_scores.get('has_diagnostic_criteria') else "‚ö†Ô∏è Implicit",
         "Guidelines"),
        
        ("Evidence Strength Levels",
         "‚ùå No",
         "‚úÖ A/B/C grades" if guidelines_scores.get('has_evidence_levels') else "‚ùå No",
         "Guidelines"),
        
        ("RAG Readiness",
         "‚úÖ QA format" if emrqa_scores.get('has_clinical_context') else "‚ö†Ô∏è Moderate",
         "‚úÖ Rich text" if guidelines_scores.get('supports_rag') else "‚ö†Ô∏è Moderate",
         "Tie"),
        
        ("Summarization Support",
         "‚úÖ Yes" if emrqa_scores.get('supports_summarization') else "‚ö†Ô∏è Moderate",
         "‚ö†Ô∏è Moderate",
         "EMR-QA"),
        
        ("Differential Diagnosis",
         "‚ö†Ô∏è Implicit" if emrqa_scores.get('has_multiple_diagnoses') else "‚ùå Limited",
         "‚úÖ Explicit" if guidelines_scores.get('has_differential_info') else "‚ö†Ô∏è Implicit",
         "Guidelines"),
    ]
    
    for criteria, emr, guide, winner in comparisons:
        print(f"{criteria:<40} {emr:<15} {guide:<15} {winner}")
    
    print("\n" + "="*100)
    print("RECOMMENDATION")
    print("="*100)
    
    print("""
üéØ BEST APPROACH: **USE BOTH DATASETS TOGETHER**

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      RECOMMENDED ARCHITECTURE                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Clinical Note      ‚îÇ
‚îÇ   (Input Text)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 1: EMR-QA (emrqa-msquad)                                 ‚îÇ
‚îÇ  Purpose: Question-answering for diagnostic reasoning            ‚îÇ
‚îÇ  ‚Ä¢ Extract key clinical facts                                   ‚îÇ
‚îÇ  ‚Ä¢ Generate diagnostic questions                                ‚îÇ
‚îÇ  ‚Ä¢ Link evidence to answers                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 2: MedCaseReasoning (Current)                            ‚îÇ
‚îÇ  Purpose: Match to similar clinical cases                       ‚îÇ
‚îÇ  ‚Ä¢ Pattern matching                                             ‚îÇ
‚îÇ  ‚Ä¢ Case-based reasoning                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LAYER 3: Clinical Guidelines (epfl-llm/guidelines)             ‚îÇ
‚îÇ  Purpose: Validate & strengthen with evidence                   ‚îÇ
‚îÇ  ‚Ä¢ Check diagnostic criteria                                    ‚îÇ
‚îÇ  ‚Ä¢ Add evidence strength (A/B/C)                                ‚îÇ
‚îÇ  ‚Ä¢ Rank by guideline support                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Differential        ‚îÇ
‚îÇ  Diagnosis Output    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

KEY INSIGHTS:

1. **EMR-QA (emrqa-msquad):**
   ‚úÖ Best for: Clinical note ‚Üí Summary
   ‚úÖ Best for: Extracting diagnostic clues
   ‚úÖ Best for: Question-answering format (good for LLM prompting)
   ‚ùå Weakness: May not have explicit differential lists

2. **Clinical Guidelines:**
   ‚úÖ Best for: Validating diagnoses with evidence
   ‚úÖ Best for: Adding strength/confidence levels
   ‚úÖ Best for: Ensuring medical accuracy
   ‚ùå Weakness: Not directly connected to patient cases

3. **Combined Approach:**
   ‚úÖ EMR-QA for understanding clinical narrative
   ‚úÖ MedCase for pattern matching
   ‚úÖ Guidelines for validation and ranking

IMPLEMENTATION PRIORITY:

Phase 1: Integrate EMR-QA for summarization
Phase 2: Keep MedCaseReasoning for differential generation
Phase 3: Add Guidelines for evidence strength and validation
""")


def save_comparison_results(emrqa_scores, guidelines_scores):
    """Save detailed comparison to file."""
    output = {
        "comparison_date": "2026-01-11",
        "goal": "Differential Diagnosis Generation",
        "datasets": {
            "emrqa_msquad": {
                "name": "Eladio/emrqa-msquad",
                "type": "EMR Question-Answering",
                "scores": emrqa_scores,
                "suitability": sum(emrqa_scores.values()) / len(emrqa_scores) * 100 if emrqa_scores else 0
            },
            "guidelines": {
                "name": "epfl-llm/guidelines",
                "type": "Clinical Practice Guidelines",
                "scores": guidelines_scores,
                "suitability": sum(guidelines_scores.values()) / len(guidelines_scores) * 100 if guidelines_scores else 0
            }
        },
        "recommendation": "Use BOTH datasets in complementary layers",
        "architecture": {
            "layer_1": "EMR-QA for summarization and fact extraction",
            "layer_2": "MedCaseReasoning for case matching (keep existing)",
            "layer_3": "Guidelines for validation and evidence strength"
        }
    }
    
    output_path = Path(__file__).parent.parent / "dataset_comparison_results.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2)
    
    print(f"\nüíæ Detailed results saved to: {output_path}")


if __name__ == "__main__":
    print("\n" + "="*100)
    print("DATASET COMPARISON FOR DIFFERENTIAL DIAGNOSIS GENERATION")
    print("="*100)
    print("\nüéØ Goal: Clinical Note Summarization + Differential Diagnosis")
    print("üìä Testing: emrqa-msquad vs epfl-llm/guidelines\n")
    
    # Test EMR-QA
    emrqa_ds, emrqa_split, emrqa_scores = load_and_analyze_emrqa()
    
    # Test Guidelines
    guidelines_ds, guidelines_split, guidelines_scores = load_and_analyze_guidelines()
    
    # Compare
    if emrqa_ds is not None or guidelines_ds is not None:
        compare_datasets(emrqa_scores, guidelines_scores)
        save_comparison_results(emrqa_scores, guidelines_scores)
    
    print(f"\n{'='*100}")
    print("COMPARISON COMPLETE!")
    print(f"{'='*100}\n")
